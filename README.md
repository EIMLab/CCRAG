# Construction Contract Retrieval-Augmentation Genervation (CC-RAG) for Question Answering (QA)
This project develops a CC-RAG framework that integrates embedding model fine-tuning, automatic chunking, and clustered chunking, while employing a bifurcated retrieval strategy. Using Chinese construction contracts as a case study, it demonstrates the construction of a knowledge base and the optimization of QA retrieval.

## 1.General Introduction
Before conducting embedding model training and text segmentation, we first convert documents related to construction contracts into structured content, drawing on their inherent hierarchical organization, which comprises chapters, sections, clauses, and items. Subsequently, we establish fundamental knowledge units that embody hierarchical relationships.

+ **1_question_generating.py**
    - We use a large language model to automatically generate single-choice and multiple-choice questions for each knowledge unit, producing a structured QA dataset for model fine-tuning.
+ **2_finetuning_embedding_model.py**
    - We build positive and negative sample pairs from the generated QA data, and fine-tune a Chinese semantic matching model to produce a domain-specific embedding model. We employ sbert-base-chinese-nli as the embedding model ([https://huggingface.co/uer/sbert-base-chinese-nli](https://huggingface.co/uer/sbert-base-chinese-nli))ã€‚
+ **3_automatic_chuking.py**
    - We embed hierarchical units using a fine-tuned embedding model and dynamically segment the text based on semantic similarity to preserve contextual integrity.
+ **4_clustered_chunking_and_QA.py**
    - Based on the automatic chunking results, a clustered chunking model is developed using HDBSCAN for hierarchical clustering and GPT-4o for generating concise summaries of each cluster, forming a multi-granularity knowledge structure.
    - A bifurcated retrieval strategy is established, dynamically selecting between document vector database or clusteed vector database to perform QA tasks.

## 2.Prerequisite
+ Prepare the Python environment along with the required libraries and their corresponding versions.

```bash
Python == 3.9.20
pandas == 2.3.3
numpy == 1.26.4
tqdm == 4.67.1
scikit-learn == 1.6.0
torch == 2.8.0
sentence-transformers == 3.3.2
datasets == 3.5.0
chromadb == 1.0.8
hdbscan == 0.8.40
openai == 1.70.0
ollama == 0.4.7
```

+ You need to prepare an API key for GPT-4o used in `1_question_generating.py` and `4_clustered_chunking_and_QA.py`.
+ To start ChromaDB, please refer to [https://docs.trychroma.com/guides/deploy/client-server-mode](https://docs.trychroma.com/guides/deploy/client-server-mode).
+ To start Ollama, please refer to [https://github.com/ollama/ollama/blob/main/docs/README.md](https://github.com/ollama/ollama/blob/main/docs/README.md).

## 3.Workflow
### (1) python: 1_question_generating.py
+ Input: `knowledge_unit.xlsx`

_Description: The knowledge units have been divided, including their hierarchical relationships._

+ Output: `question_generating_for_finetuning_embedding_model.xlsx`

_Description: A dataset of knowledge units and questions generated by GPT._

### (2) python: 2_finetuning_embedding_model.py
+ Input: `question_generating_for_finetuning_embedding_model.json`

_Description: The output obtained from step (1)._

+ Output: `output_folder/finetuning_embedding_model/`

_Description: This is intended for selecting the best-performing fine-tuned embedding model for chunking, namely the selected_finetuned_embedding_model used in the subsequent two sections._

### (3) python: 3_automatic_chuking.py
+ Input: `knowledge_unit.xlsx`

_Description: The knowledge units have been divided, including their hierarchical relationships._

+ Output: `automatic_chunk_for_cluster.xlsx`

_Description: Chunk clustering results based on sequential relationships._

### (**4) python: 4_clustered_chunking_and_QA.py**
+ Input 1: `automatic_chunk_for_cluster.xlsx`

_Description: The output obtained from step (3)._

+ Output 1: `cluster_chunk_results.json` 

_Description: Hierarchical clustering results of chunks across different sections are presented._



+ Input 2: `test_set.xlsx`

_Description: Test set is used for evaluating the effectiveness of CC-RAG._

+ Output 2: `Answer Results.xlsx` 

_Description: There are the QA results of the tested LLMs, including questions, answers, and related information._

## 


